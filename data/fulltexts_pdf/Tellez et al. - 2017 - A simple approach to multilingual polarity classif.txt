A Simple Approach to Multilingual Polarity Classification in
Twitter
Eric S. Tellez
∗†
eric.tellez@infotec.mx
Sabino Miranda-Jim´enez
∗†
sabino.miranda@infotec.mx
Mario Graff
∗†
mario.graff@infotec.mx
Daniela Moctezuma
∗‡
dmoctezuma@centrogeo.edu.mx
Ranyart R. Su´arez
§
ranyart@dep.fie.umich.mx
Oscar S. Siordia
‡
osanchez@centrogeo.edu.mx
Sept.
2016
Abstract
Recently, sentiment analysis has received a lot of attention due to the interest in mining opinions of
social media users.
Sentiment analysis consists in determining the polarity of a given text, i.e., its degree
of
positiveness or negativeness.
Traditionally,
Sentiment Analysis algorithms have been tailored to a
specific language given the complexity of having a number of lexical variations and errors introduced by
the people generating content.
In this contribution, our aim is to provide a simple to implement and easy
to use multilingual framework, that can serve as a baseline for sentiment analysis contests, and as starting
point to build new sentiment analysis systems.
We compare our approach in eight different languages,
three of them have important international
contests,
namely,
SemEval
(English),
TASS (Spanish),
and
SENTIPOLC (Italian).
Within the competitions our approach reaches from medium to high positions
in the rankings; whereas in the remaining languages our approach outperforms the reported results.
1
Introduction
Sentiment analysis is a crucial task in opinion mining field where the goal is to extract opinions, emotions,
or attitudes to different entities (person,
objects,
news,
among others).
Clearly,
this task is of interest for
all
languages;
however,
there exists a significant gap between English state-of-the-art methods and other
languages.
It is expected that some researchers decided to test the straightforward approach which consists
in,
first,
translating the messages to English,
and,
then,
use a high performing English sentiment classifier
(for instance,
see
[3]
and
[4]) instead of
creating a sentiment classifier optimized for a given language.
However,
the advantages of a properly tuned sentiment classifier have been studied for different languages
(for instance, see [18, 25, 1, 2]).
This manuscript focuses on the particular case of multilingual sentiment analysis of short informal texts
such as Twitter messages.
Our aim is to provide an easy-to-use tool
to create sentiment classifiers based
∗
CONACyT Consejo Nacional
de Ciencia y Tecnolog´ıa,
Direcci´
on de C´atedras,
Insurgentes Sur 1582,
Cr´edito Constructor
03940, Ciudad de M´exico, M´exico.
†
INFOTEC Centro de Investigaci´on e Innovaci´on en Tecnolog´ıas de la Informaci´
on y Comunicaci´on, Circuito Tecnopolo Sur
No 112, Fracc.
Tecnopolo Pocitos II, Aguascalientes 20313, M´exico.
‡
Centro de Investigaci´
on en Geograf´ıa y Geom´atica “Ing.
Jorge L. Tamayo”, A.C. Circuito Tecnopolo Norte No.
117, Col.
Tecnopolo Pocitos II, C.P. 20313,.
Aguascalientes, Ags, M´exico.
§
Departamento de Estudios de Posgrado,
Facultad de Ingenier´ıa El´ectrica,
Universidad Michoacana de San Nicol´as de
Hidalgo, Santiago Tapia 403, Morelia 58000, M´exico
1
arXiv:1612.05270v1 [cs.CL] 15 Dec 2016
on supervised learning (i.e.,
labeled dataset) where the classifier should be competitive to those sentiment
classifiers carefully tuned by some given languages.
Furthermore,
our second contribution is to create a
well-performing baseline to compare new sentiment classifiers in a broad range of languages or to bootstrap
new sentiment analysis systems.
Our approach is based on selecting the best text-transforming techniques
that optimize some performance measures where the chosen techniques are robust to typical writing errors.
In this context,
we propose a robust multilingual
sentiment analysis method,
tested in eight different
languages:
Spanish,
English,
Italian,
Arabic,
German,
Portuguese,
Russian and Swedish.
We compare
our approach ranking in three international
contests:
TASS’15,
SemEval’15-16 and SENTIPOLC’14,
for
Spanish,
English and Italian respectively;
the remaining languages are compared directly with the results
reported in the literature.
The experimental results locate our approach in good positions for all considered
competitions;
and excellent results in the other five languages tested.
Finally,
even when our method is
almost cross-language,
it can be extended to take advantage of
language dependencies;
we also provide
experimental evidence of the advantages of using these language-dependent techniques.
The rest of the manuscript is organized as follows.
Section 2 describes our proposed Sentiment Analysis
method.
Section 3 describes the datasets and contests used to test our approach; whereas, the experimental
results, and, the discussion are presented on Section 4.
Finally, Section 5 concludes.
2
Our Approach:
Multilingual Polarity Classification
We propose a method for multilingual polarity classification that can serve as a baseline as well as a framework
to build more complex sentiment analysis systems due to its simplicity and availability as an open source
software
1
.
As we mentioned,
this baseline algorithm for multilingual
Sentiment Analysis (B4MSA) was
designed with the purpose of
being multilingual
and easy to implement.
B4MSA is not a na¨ıve baseline
which is experimentally proved by evaluating it on several international competitions.
In a nutshell,
B4MSA starts by applying text-transformations to the messages,
then transformed text
is represented in a vector space model
(see Subsection 2.3),
and finally,
a Support Vector Machine (with
linear kernel) is used as the classifier.
B4MSA uses a number of text transformations that are categorized
in cross-language features (see Subsection 2.1) and language dependent features (see Subsection 2.2).
It is
important to note that, all the text-transformations considered are either simple to implement or there is a
well-known library (e.g.
[9,
23]) to use them.
It is important to note that to maintain the cross-language
property, we limit ourselves to not use additional knowledge, this include knowledge from affective lexicons
or models based on distributional semantics.
To obtain the best performance,
one needs to select those text-transformations that work best for a
particular dataset, therefore, B4MSA uses a simple random search and hill-climbing (see Subsection 2.4) in
space of text-transformations to free the user from this delicate and time-consuming task.
Before going into
the details of each text-transformation, Table 1 gives a summary of the text-transformations used as well as
their parameters associated.
2.1
Cross-language Features
We defined cross-language features as a set of
features that could be applied in most similar languages,
not only related language families such as Germanic languages (English, German, etc.), Romance languages
(Spanish, Italian, etc.), among others; but also similar surface features such as punctuation, diacritics, symbol
duplication,
case sensitivity,
etc.
Later,
the combination of these features will be explored to find the best
configuration for a given classifier.
2.1.1
Spelling Features
Generally,
Twitter messages are full
of
slang,
misspelling,
typographical
and grammatical
errors among
others;
in order to tackle these aspects we consider different parameters to study this effect.
The following
1
https://github.com/INGEOTEC/b4msa
2
Table 1:
Parameter list and a brief description of the functionality
cross-language features
name
values
description
del-d1
yes, no
If it is enabled then the sequences of repeated symbols
are replaced by a single occurrence of the symbol.
del-diac
yes, no
Determines if diacritic symbols,
e.g.,
accent symbols,
should be removed from the text.
emo
remove, group, none
Controls how emoticons are handled,
i.e.
removed,
grouped by expressed emotion, or nothing.
num
remove, group, none
Controls
how numbers
are
handled,
i.e.,
removed,
grouped into a special tag, or nothing.
url
remove, group, none
Controls
how URLs
are
handled,
i.e.,
removed,
grouped into a special tag, or nothing.
usr
remove, group, none
Controls how users are handled, i.e., removed, grouped
into a special tag, or nothing.
lc
yes, no
Letters are normalized to be lowercase if it is enabled
tokenizer
P(n-words ∪ q-grams)
One item among the power set (discarding the empty-
set) of the union of *n-words and *q-grams.
*n-words
{1, 2}
The number of words used to describe a token.
*q-grams
{1, 2, 3, 4, 5, 6, 7}
The length in characters of a token.
language dependent features
name
values
description
stem
yes, no
Determines if words are stemmed.
neg
yes, no
Determines if negation operators in the text are nor-
malized and directly connected with the next content
word.
sw
remove, group, none
Controls
how stopwords
are handled,
i.e.,
removed,
grouped, or left untouched.
:)
:D
:P
→ pos
:(
:-(
:’(
→ neg
:-|
U U
-.-
→ neu
Table 2:
An excerpt of the mapping table from Emoticons to its polarity words.
points are the parameters to be considered as spelling features.
Punctuation (del-punc) considers the use of
symbols such as question mark, period, exclamation point, commas, among other spelling marks.
Diacritic
symbols (del-diac) are commonly used in languages such as Spanish,
Italian,
Russian,
etc.,
and its wrong
usage is one of the main sources of orthographic errors in informal texts; this parameter considers the use or
absence of diacritical
marks.
Symbol
reduction (del-d1 ),
usually,
twitter messages use repeated characters
to emphasize parts of the word to attract user’s attention.
This aspect makes the vocabulary explodes.
We
applied the strategy of
replacing the repeated symbols by one occurrence of
the symbol.
Case sensitivity
(lc) considers letters to be normalized in lowercase or to keep the original source; the aim is to cut the words
that are the same in uppercase and lowercase.
2.1.2
Emoticon (emo) Feature
We classified around 500 most popular emoticons,
included text emoticons,
and the whole set of
unicode
emoticons (around 1, 600) defined by [27] into three classes:
positive, negative and neutral, which are grouped
under its corresponding polarity word defined by the class name.
Table 2 shows an excerpt of the dictionary that maps emoticons to their corresponding polarity class.
3
2.1.3
Word-based n-grams (n-words) Feature
N-words (word sequences) are widely used in many NLP tasks, and they have also been used in Sentiment
Analysis [26]
and [11].
To compute the N-words,
the text is tokenized and N-words are calculated from
tokens.
For example,
let T = "the lights and shadows of your future" be the text,
so its 1-words
(unigrams) are each word alone, and its 2-words (bigrams) set are the sequences of two words, the set (W
T
2
),
and so on.
W
T
2
= {
the lights, lights and, and shadows, shadows of, of your, your future
},
so,
given
text of size m words, we obtain a set containing at most m − n + 1 elements.
Generally, N-words are used
up to 2 or 3-words because it is uncommon to find, between texts, good matches of word sequences greater
than three or four words [14].
2.1.4
Character-based q-grams (q-grams)
In addition to the traditional N-words representation, we represent the resulting text as q-grams.
A q-grams
is an agnostic language transformation that consists in representing a document by all its substring of length
q.
For example, let T = “abra cadabra” be the text, its 3-grams set are
Q
T
3
= {abr, bra, ra , a c,
ca, aca, cad, ada, dab},
so,
given text of
size m characters,
we obtain a set with at most m − q + 1 elements.
Notice that this
transformation handles white-spaces as part of the text.
Since there will
be q-grams connecting words,
in
some sense, applying q-grams to the entire text can capture part of the syntactic and contextual information
in the sentence.
The rationale of q-grams is also to tackle misspelled sentences from the approximate pattern
matching perspective [21].
2.2
Language Dependent Features
The following features are language dependent because they use specific information from the language
concerned.
Usually,
the use of
stopwords,
stemming and negations are traditionally used in Sentiment
Analysis.
The users of this approach could add other features such as part of speech, affective lexicons, etc.
to improve the performance [16].
2.2.1
Stopwords (sw) Feature
In many languages,
there is a set of extremely common words such as determiners or conjunctions (the or
and) which help to build sentences but do not carry any meaning for themselves.
These words are known as
Stopwords, and they are removed from text before any attempt to classify them.
Generally, a stopword list
is built using the most frequent terms from a huge document collection.
We used the Spanish, English and
Italian stopword lists included in the NLTK Python package [9] in order to identify them.
2.2.2
Stemming (stem) Feature
Stemming is a well-known heuristic process in Information Retrieval field that chops off the end of words and
often includes the removal of derivational affixes.
This technique uses the morphology of the language coded
in a set of rules that are applied to find out word stems and reduce the vocabulary collapsing derivationally
related words.
In our study, we use the Snowball Stemmer for Spanish and Italian, and the Porter Stemmer
for English that are implemented in NLTK package [9].
2.2.3
Negation (neg) Feature
Negation markers might change the polarity of
the message.
Thus,
we attached the negation clue to the
nearest word,
similar to the approaches used in [26].
A set of
rules was designed for common negation
structures that involve negation markers for Spanish,
English and Italian.
For instance,
negation markers
used for Spanish are no (not), nunca, jam´as (never), and sin (without).
The rules (regular expressions) are
4
processed in order,
and their purpose is to negate the nearest word to the negation marker using only the
information on the text, e.g., avoiding mainly pronouns and articles.
For example, in the sentence El
coche
no es bonito (The car is not nice), the negation marker no and not (for English) is attached to its adjective
no bonito (not nice).
2.3
Text Representation
After text-transformations,
it is needed to represent the text in suitable form in order to use a traditional
classifier such as SVM. It was decided to select the well known vector representation of a text given its sim-
plicity and powerful representation.
Particularly, it is used the Term Frequency-Inverse Document Frequency
which is a well-known weighting scheme in NLP. TF-IDF computes a weight that represents the importance
of words or terms inside a document in a collection of documents,
i.e.,
how frequently they appear across
multiple documents.
Therefore,
common words such as the and in,
which appear in many documents,
will
have a low score, and words that appear frequently in a single document will have high score.
This weighting
scheme selects the terms that represent a document.
2.4
Parameter Optimization
The model selection, sometimes called hyper-parameter optimization, is essential to ensure the performance
of a sentiment classifier.
In particular,
our approach is highly parametric;
in fact,
we use such property to
adapt to several
languages.
Table 1 summarizes the parameters and their valid values.
The search space
contains more than 331 thousand configurations when limited to multilingual
and language independent
parameters; while the search space reaches close to 4 million configurations when we add our three language-
dependent parameters.
Depending on the size of the training set, each configuration needs several minutes
on a commodity server to be evaluated; thus, an exhaustive exploration of the parameter space can be quite
expensive making the approach useless in practice.
To tackle the efficiency problems, we perform the model
selection using two hyper-parameter optimization algorithms.
The first corresponds to Random Search, described in depth in [8].
Random search consists on randomly
sampling the parameter space and select the best configuration among the sample.
The second algorithm
consists on a Hill
Climbing [10,
7]
implemented with a memory to avoid testing a configuration twice.
The main idea behind hill
climbing H+M is to take a pivoting configuration,
explore the configuration’s
neighborhood,
and greedily moves to the best neighbor.
The process is repeated until
no improvement is
possible.
The configuration neighborhood is defined as the set of configurations such that these differ in just
one parameter’s value.
This rule is strengthened for tokenizer (see Table 1) to differ in a single internal value
not in the whole parameter value.
More precisely,
let t be a valid value for tokenizer and N (t) the set of
valid values for neighborhoods of t, then |t ∪ s| ∈ {|t|, |t| + 1} and |t ∩ s| ∈ {|t|, |t| − 1} for any s ∈ N (t).
To guarantee a better or equal
performance than random search,
the H+M process starts with the
best configuration found in the random search.
By using H+M,
sample size can be set to 32 or 64,
as
rule of
thumb,
and even reach improvements in most cases (see §4).
Nonetheless,
this simplification and
performance boosting comes along with possible higher optimization times.
Finally, the performance of each
configuration is obtained using a cross-validation technique on the training data, and the metrics are usually
used in classification such as:
accuracy, score F
1
, and recall, among others.
3
Datasets and contests
Nowadays,
there are several
international
competitions related to text mining,
which include diverse tasks
such as:
polarity classification (at different levels), subjectivity classification, entity detection, and iron de-
tection, among others.
These competitions are relevant to measure the potential of different proposed tech-
niques.
In this case, we focused on polarity classification task, hence, we developed a baseline method with
an acceptable performance achieved in three different contests, namely, TASS’15 (Spanish) [28], SemEval’15-
16 (English) [24,
20],
and SENTIPOLC’14 (Italian) [6].
In addition,
our approach was tested with other
5
Table 3:
Datasets details from each competition tested in this work
Dataset
Positive
Neutral
Negative
None
Total
SemEval’15
Training
2,800
3,661
1,060
-
7,521
(English)
Development
446
580
262
-
1,288
Gold
841
824
298
-
1,963
SemEval’16
Training
3,094
2,043
863
-
6,000
(English)
Development
844
765
391
-
2,000
Gold
7,059
10,342
3,231
-
20,632
TASS’15
Training
2,884
670
2,182
1,482
7,218
(Spanish)
Development
-
-
-
-
-
Gold 1K
363
22
268
347
1,000
Gold 60K
22,233
1,305
15,844
21,416
60,798
SENTIPOL’14
Training
969
320
1,671
1,541
4,501
(Spanish)
Development
-
-
-
-
-
Gold
453
113
754
607
1,927
Arabic [18, 25]
Unique
448
202
1,350
-
2,000
German [19]
Unique
23,860
50,368
17,274
-
91,502
Portuguese [19]
Unique
24,595
29,357
32,110
-
86,062
Russian [19]
Unique
19,238
28,665
21,197
-
69,100
Swedish [19]
Unique
13,265
15,410
20,580
-
49,255
languages (Arabic, German, Portuguese, Russian, and Swedish) to show that is feasible to use our framework
as basis for building more complex sentiment analysis systems.
From these languages, datasets and results
can be seen in [19], [25] and [18].
Table 3 presents the details of each of the competitions considered as well as the other languages tested.
It
can be observed, from the table, the number of examples as well as the number of instances for each polarity
level,
namely,
positive,
neutral,
negative and none.
The training and development (only in SemEval) sets
are used to train the sentiment classifier,
and the gold set is used to test the classifier.
In the case there
dataset was not split in training and gold (Arabic, German, Portuguese, Russian, and Swedish) then a cross-
validation (10 folds) technique is used to test the classifier.
The performance of the classifier is presented
using different metrics depending the competition.
SemEval
uses the average of
score F
1
of
positive and
negative labels, TASS uses the accuracy and SENTIPOLC uses a custom metric (see [28, 24, 20, 6]).
4
Experimental Results
We tested our framework on two kinds of
datasets.
On one hand,
we compare our performance on three
languages having well known sentiment analysis contests; here, we compare our work against competitors of
those challenges.
On the other hand,
we selected five languages without popular opinion mining contests;
for these languages, we compare our approach against research works reporting the used corpus.
4.1
Performance on sentiment analysis contests
Figure 1 shows the performance on four contests,
corresponding to three different languages.
The perfor-
mance corresponds to the multilingual set of features, i.e., we do not used language-dependent techniques.
Figures 1(a)-1(d) illustrates the results on each challenge, all competitors are ordered in score’s descending
order (higher is better).
The achieved performance of our approach is marked with a horizontal line on each
figure.
Figure 1(e) briefly describes each challenge and summarizes our performance on each contest;
also,
we added three standard measures to simplify the insight’s creation of the reader.
6
The winner method in SENTIPOLC’14 (Italian) is reported in
[5].
This method uses three groups of
features:
keyword and micro-blogging characteristics, Sentiment Lexicons, SentiWordNet and MultiWordNet,
and Distributional Semantic Model (DSM) with a SVM classifier.
In contrast with our method, in [5] three
external sentiment lexicons dictionaries were employed; that is, external information.
In TASS’15 (Spanish) competition, the winner reported method was
[17], which proposed an adaptation
based on a tokenizer of tweets Tweetmotif [15], Freeling [22] as lemmatizer, entity detector, morphosyntactic
labeler and a translation of the Afinn dictionary.
In contrast with our method, [17] employs several complex
and expensive tools.
In this task we reached the fourteenth position with an accuracy of 0.637.
Figure 1(b)
shows the B4MSA performance to be over two thirds of the competitors.
The remaining two contests correspond to the SemEval’15-16.
The B4MSA performance in SemEval
is
depicted in Figures 1(c) and 1(d);
here,
B4MSA does not perform as well
as in other challenges,
mainly
because, contrary to other challenges, SemEval rules promotes the enrichment of the official training set.
To
be consistent with the rest of the experiments,
B4MSA uses only the official
training set.
The results can
be significantly improved using larger training datasets;
for example,
joining SemEval’13 and SemEval’16
training sets, we can reach 0.54 for SemEval’16, which improves the B4MSA’s performance (see Table 1).
In SemEval’15, the winner method is
[12], which combines three approaches among the participants of
SemEval’13,
teams:
NRC-Canada,
GU-MLT-LT and KLUE,
and from SemEval’14 the participant TeamX
all of them employing external information.
In SemEval’16, the winner method was
[13] is composed with
an ensemble of two subsystems based on convolutional neural networks, the first subsystem is created using
290 million tweets, and the second one is feeded with 150 million tweets.
All these tweets were selected from
a very large unlabeled dataset through distant supervision techniques.
rank
0.35
0.40
0.45
0.50
0.55
0.60
0.65
0.70
score
(a) SENTIPOLC’14
rank
0.40
0.45
0.50
0.55
0.60
0.65
0.70
0.75
score
(b) TASS’15
rank
0.20
0.25
0.30
0.35
0.40
0.45
0.50
0.55
0.60
0.65
score
(c) SemEval’15
rank
0.25
0.30
0.35
0.40
0.45
0.50
0.55
0.60
0.65
score
(d) SemEval’16
name
language
classes
challenge’s score
rank
score
acc.
macro F
1
(F
pos
1
+ F
neg
1
) /2
SENTIPOLC’14
italian
{pos, neg, none, mix}
custom (see [6])
2 / 14
0.677
0.604
0.599
0.599
TASS’15
spanish
{pos, neg, neu, none}
accuracy
14 / 40
0.637
0.637
0.498
0.697
SemEval’15
english
{pos, neg, neu}
(F
pos
1
+ F
neg
1
) /2
34 / 42
0.534
0.629
0.584
0.534
SemEval’16
english
{pos, neg, neu}
(F
pos
1
+ F
neg
1
) /2
28 / 36
0.454
0.534
0.477
0.454
(e) B4MSA’s performance summary; the last columns correspond to three standard scores.
Figure 1:
The performance listing in four difference challenges.
The horizontal
lines appearing in a) to
d) correspond to B4MSA’s performance.
All scores were computed using the official gold-standard and the
proper score for each challenge.
Table 4 shows the multilingual
set of
techniques and the set with language-dependent techniques;
for
each,
we optimized the set of
parameters through Random Search and H + M (see Subsection 2.4).
The
reached performance is reported using both cross-validation and the official
gold-standard.
Please notice
how H + M consistently reaches better performances,
even on small
sampling sizes.
The sampling size is
indicated with subscripts in Table 4.
Note that,
in SemEval
challenges,
the cross-validation performances
are higher than those reached by evaluating the gold-standard,
mainly because the gold-standard does not
7
follow the distribution of training set.
This can be understood because the rules of SemEval
promote the
use of external knowledge.
Table 4:
B4MSA’s performance on cross-validation and gold standard.
The subscript at right of each score
means for the random-search’s parameter (sampling size) needed to find that value.
Dataset
Multilingual Parameters
Language-Dependent Parameters
Random Search
H+M Search
Random Search
H+M Search
cross-val.
gold-std.
cross-val.
gold-std.
cross-val.
gold-std.
cross-val.
gold-std.
SENTIPOLC’14
-
0.678
256
-
0.677
16
-
0.675
8
-
0.674
256
TASS’15
0.643
128
0.636
128
0.648
8
0.637
8
0.644
256
0.635
256
0.649
32
0.637
32
SemEval’15
0.585
256
0.530
256
0.590
8
0.534
8
0.590
256
0.520
256
0.596
128
0.528
128
SemEval’16
0.575
64
0.456
64
0.578
64
0.454
64
0.580
256
0.462
256
0.583
256
0.462
256
Table 5 compares our performance on five different languages;
we do not apply language-dependent
techniques.
For each comparison,
we took a labeled corpus from [25]
(Arabic) and [19]
(the remaining
languages).
According to author’s reports, all tweets were manually labeled by native speakers as pos, neg,
or neu.
The Arabic dataset contains 2, 000 items; the other datasets contain from 58 thousand tweets to more
than 157 thousand tweets.
We were able to fetch a fraction of the original datasets; so, we drop the necessary
items to hold the original class-population ratio.
The ratio of tweets in our training dataset, respect to the
original dataset, is indicated beside the name.
As before, we evaluate our algorithms through a 10-fold cross
validation.
In [25,
18],
the authors study the effect of translation in sentiment classifiers;
they found better to use
native Arabic speakers as annotators than fine-tuned translators plus fine-tuned English sentiment classifiers.
In [19], the idea is to measure the effect of the agreement among annotators on the production of a sentiment-
analysis corpus.
Both,
on the technical
side,
both papers use fine tuned classifiers plus a variety of
pre-
processing techniques to prove their claims.
Table 5 supports the idea of choosing B4MSA as a bootstrapping
sentiment classifier because, in the overall, B4MSA reaches superior performances regardless of the language.
Our approach achieves those performance’s levels since it optimizes a set of
parameters carefully selected
to work on a variety of languages and being robust to informal writing.
The latter problem is not properly
tackled in many cases.
Table 5:
Performance on multilingual sentiment analysis (not challenges).
B4MSA was restricted to use only
the multilingual set of parameters.
language
F
1
(F
pos
1
+ F
neg
1
) /2
acc
Arabic
Salameh et al. [25]
-
-
0.787
Saif et al. [18]
-
-
0.794
B4MSA (100%)
0.642
0.781
0.799
German
Mozetiˇc et al. [19]
-
0.536
0.610
B4MSA (89%)
0.621
0.559
0.668
Portuguese
Mozetiˇc et al. [19]
-
0.553
0.507
B4MSA (58%)
0.550
0.591
0.555
Russian
Mozetiˇc et al. [19]
-
0.615
0.603
B4MSA (69%)
0.754
0.768
0.750
Swedish
Mozetiˇc et al. [19]
-
0.657
0.616
B4MSA (93%)
0.680
0.717
0.691
8
5
Conclusions
We presented a simple to implement multilingual framework for polarity classification whose main contribu-
tions are in two aspects.
On one hand, our approach can serve as a baseline to compare other classification
systems.
It considers techniques for text representation such as spelling features,
emoticons,
word-based
n-grams,
character-based q-grams and language dependent features.
On the other hand,
our approach is a
framework for practitioners or researchers looking for a bootstrapping sentiment classifier method in order
to build more elaborated systems.
Besides the text-transformations, the proposed framework uses a SVM classifier (with linear kernel), and,
hyper-parameter optimization using random search and H+M over the space of text-transformations.
The
experimental
results show good overall
performance in all
international
contests considered,
and the best
results in the other five languages tested.
It is important to note that all the methods that outperformed B4MSA in the sentiment analysis contests
use extra knowledge (lexicons included) meanwhile B4MSA uses only the information provided by each
contests.
In future work,
we will
extend our methodology to include extra-knowledge in order to improve
the performance.
Acknowledgements
We would like to thank Valerio Basile, Julio Villena-Roman, and Preslav Nakov for kindly give us access to
the gold-standards of SENTIPOLC’14, TASS’15 and SemEval 2015 & 2016, respectively.
The authors also
thank Elio Villase˜
nor for the helpful discussions in early stages of this research.
References
[1]
Matheus Araujo,
Julio Reis,
Adriano Pereira,
and Fabricio Benevenuto.
An evaluation of
machine
translation for multilingual
sentence-level
sentiment analysis.
In Proceedings of the 31st Annual
ACM
Symposium on Applied Computing, SAC ’16, pages 1140–1145, New York, NY, USA, 2016. ACM.
[2]
Daniella Bal,
Malissa Bal,
Arthur van Bunningen,
Alexander Hogenboom,
Frederik Hogenboom,
and
Flavius Frasincar.
Sentiment
Analysis with a Multilingual
Pipeline,
pages 129–142.
Springer Berlin
Heidelberg, Berlin, Heidelberg, 2011.
[3]
Alexandra Balahur and Marco Turchi.
Multilingual sentiment analysis using machine translation?
In
Proceedings of the 3rd Workshop in Computational
Approaches to Subjectivity and Sentiment Analysis,
WASSA ’12, pages 52–60, Stroudsburg, PA, USA, 2012. Association for Computational Linguistics.
[4]
Alexandra Balahur and Marco Turchi. Comparative experiments using supervised learning and machine
translation for multilingual sentiment analysis.
Computer Speech & Language, 28(1):56 – 75, 2014.
[5]
Pierpaolo Basile and Nicole Novielli.
Uniba at evalita 2014-sentipolc task:
Predicting tweet sentiment
polarity combining micro-blogging, lexicon and semantic features.
In Proceedings of the 4th evaluation
campaign of Natural Language Processing and Speech tools for Italian (EVALITA’14), Pisa, Italy, 2014.
[6]
Valerio Basile, Andrea Bolioli, Malvina Nissim, Viviana Patti, and Paolo Rosso. Overview of the Evalita
2014 SENTIment POLarity Classification Task. In Proceedings of the 4th evaluation campaign of Natural
Language Processing and Speech tools for Italian (EVALITA’14), Pisa, Italy, 2014.
[7]
Roberto Battiti,
Mauro Brunato,
and Franco Mascia.
Reactive search and intelligent
optimization,
volume 45.
Springer Science & Business Media, 2008.
[8]
James Bergstra and Yoshua Bengio.
Random search for hyper-parameter optimization.
Journal
of
Machine Learning Research, 13(Feb):281–305, 2012.
9
[9]
Steven Bird,
Ewan Klein,
and Edward Loper.
Natural
Language Processing with Python.
O’Reilly
Media, 2009.
[10]
Edmund K Burke, Graham Kendall, et al.
Search methodologies.
Springer, 2005.
[11]
Zhijian Cui, Xiaodong Shi, and Yidong Chen.
Sentiment analysis via integrating distributed represen-
tations of variable-length word sequence.
Neurocomputing, 2015.
[12]
Matthias Hagen,
Martin Potthast,
Michel
Bchner,
and Benno Stein.
Webis:
an ensemble for twitter
sentiment detection. In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval
2015, pages 582–589. Association for Computational Linguistics, June 2015.
[13]
Jan Deriu; Maurice Gonzenbach; Fatih Uzdilli; Aurelien Lucchi; Valeria De Luca; Martin Jaggi.
Swiss-
cheese at semeval-2016 task 4:
Sentiment classification using an ensemble of convolutional neural net-
works with distant supervision.
In Proceedings of the 10th International
Workshop on Semantic Evalu-
ation, SemEval ’16, San Diego, California, June 2016. Association for Computational Linguistics.
[14]
Daniel Jurafsky and James H. Martin.
Speech and Language Processing (2Nd Edition).
Prentice-Hall,
Inc., Upper Saddle River, NJ, USA, 2009.
[15]
Michel
Krieger and David Ahn.
Tweetmotif:
exploratory search and topic summarization for twitter.
In In Proc. of AAAI Conference on Weblogs and Social, 2010.
[16]
Bing Liu.
Sentiment
Analysis:
Mining Opinions,
Sentiments,
and Emotions.
Cambridge University
Press, 2015.
ISBN: 1-107-01789-0. 381 pages.
[17]
Ferran Pla Llu´ıs-F Hurtado and Davide Buscaldi.
Elirf-upv en tass 2015:
Anlisis de sentimientos en
twitter.
In TASS 2015:
Workshop on Sentiment Analysis at SEPLN co-located with 31st SEPLN Con-
ference (SEPLN 2015), pages 75–79. Villena Rom´
an, Julio; Garc´ıa Morera, Janine; Garc´ıa Cumbreras,
Miguel
´
Angel; Mart´ınez C´
amara, Eugenio; Mart´ın Valdivia, M. Teresa; Ure˜
na L´opez, L. Alfonso, 2015.
[18]
Saif M. Mohammad, Mohammad Salameh, and Svetlana Kiritchenko. How translation alters sentiment.
Journal
of Artificial
Intelligence Research, 55:95–130, 2016.
[19]
Igor Mozetiˇc,
Miha Grˇcar,
and Jasmina Smailovi´c.
Multilingual
twitter sentiment classification:
The
role of human annotators.
PloS one, 11(5):e0155036, 2016.
[20]
Preslav Nakov, Alan Ritter, Sara Rosenthal, Veselin Stoyanov, and Fabrizio Sebastiani.
SemEval-2016
task 4:
Sentiment analysis in Twitter.
In Proceedings of the 10th International
Workshop on Semantic
Evaluation, SemEval ’16, San Diego, California, June 2016. Association for Computational Linguistics.
[21]
G. Navarro and M. Raffinot.
Flexible Pattern Matching in Strings – Practical
on-line search algorithms
for texts and biological
sequences.
Cambridge University Press, 2002.
ISBN 0-521-81307-7. 280 pages.
[22]
Llus Padr and Evgeny Stanilovsky.
Freeling 3.0:
Towards wider multilinguality.
In Proceedings of the
Language Resources and Evaluation Conference (LREC 2012), Istanbul, Turkey, May 2012. ELRA.
[23]
Radim
ˇ
Reh˚uˇrek and Petr Sojka.
Software Framework for Topic Modelling with Large Corpora.
In Pro-
ceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks, pages 45–50, Valletta,
Malta, May 2010. ELRA.
http://is.muni.cz/publication/884893/en.
[24]
Sara Rosenthal, Preslav Nakov, Svetlana Kiritchenko, Saif Mohammad, Alan Ritter, and Veselin Stoy-
anov.
Semeval-2015 task 10:
Sentiment analysis in twitter.
In Proceedings of
the 9th International
Workshop on Semantic Evaluation (SemEval
2015), pages 451–463, Denver, Colorado, June 2015. As-
sociation for Computational Linguistics.
10
[25]
Mohammad Salameh, Saif Mohammad, and Svetlana Kiritchenko.
Sentiment after translation:
A case-
study on arabic social
media posts.
In Proceedings of
the 2015 Conference of
the North American
Chapter of the Association for Computational
Linguistics:
Human Language Technologies,
pages 767–
777, Denver, Colorado, May–June 2015. Association for Computational Linguistics.
[26]
Grigori Sidorov, Sabino Miranda-Jim´enez, Francisco Viveros-Jim´enez, Alexander Gelbukh, No´e Castro-
S´
anchez, Francisco Vel´
asquez, Ismael D´ıaz-Rangel, Sergio Su´arez-Guerra, Alejandro Trevi˜
no, and Juan
Gordon. Empirical study of machine learning based approach for opinion mining in tweets. In Proceedings
of
the 11th Mexican International
Conference on Advances in Artificial
Intelligence - Volume Part I,
MICAI’12, pages 1–14, Berlin, Heidelberg, 2013. Springer-Verlag.
[27]
Unicode.
Unicode emoji
chart.
http://unicode.org/emoji/charts/full-emoji-list.html,
2016.
Accessed 20-May-2016.
[28]
Julio
Villena
Rom´
an,
Janine
Garc´ıa
Morera,
Miguel
´
Angel
Garc´ıa
Cumbreras,
Eugenio
Mart´ınez C´amara,
M.
Teresa Mart´ın Valdivia,
and L.
Alfonso Ure˜
na L´
opez.
Overview of
tass 2015.
In TASS 2015:
Workshop on Sentiment Analysis at SEPLN co-located with 31st SEPLN Conference
(SEPLN 2015), pages 13–21. Villena Rom´
an, Julio; Garc´ıa Morera, Janine; Garc´ıa Cumbreras, Miguel
´
Angel; Mart´ınez C´amara, Eugenio; Mart´ın Valdivia, M. Teresa; Ure˜
na L´
opez, L. Alfonso, 2015.
11
