{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA visualization of gensim citations\n",
    "\n",
    "### A visualization of citations of \"Software Framework for Topic Modelling with Large Corpora\" represented as LDA probability distribution towards the selected number of topics.\n",
    "\n",
    "### Note the configurable parameters to play with:\n",
    "\n",
    "#### Preprocessing:\n",
    "\n",
    "* Text filter: Whether to filter out some part of texts, or not\n",
    "* Filter parameters (for filtering of HTML content) based on: \n",
    "    * minimal length of a valid text sentence, \n",
    "    * minimal length of a valid text line\n",
    "* Text preprocess method - currently gensim's preprocess_text() - works the best only for English\n",
    "\n",
    "#### LDA:\n",
    "\n",
    "* Number of topics\n",
    "* Number of passes over the input corpus\n",
    "* Many others, so far left on defaults: https://radimrehurek.com/gensim/models/ldamodel.html\n",
    "\n",
    "#### Tf-Idf:\n",
    "\n",
    "* Corpus representation: 1. BoW counts of words, or 2. Tf-Idf weights\n",
    "* Representative term set filtering: to consider only a given percentile of top-important terms according to tf-idf weights for each doc\n",
    "\n",
    "#### Visualization:\n",
    "\n",
    "* Docs representation: currently over a distance matrix\n",
    "    * Distance method for distance matrix: correlation, cosine, euclidean, ... choose from https://docs.scipy.org/doc/scipy/reference/spatial.distance.html\n",
    "* Dimensionality reduction: currently MDS from distance matrix\n",
    "\n",
    "#### Topic representation in visualization:\n",
    "\n",
    "* Number of most important words for topic\n",
    "* For each word from a set of each topic, maximum number of occurrences of this word in other clusters representations\n",
    "    * Can clarify the meaning of the topic in contrast to other inferred topics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "from gensim.models import TfidfModel\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data load:\n",
    "\n",
    "1. using filter for short sentences (to get rid of HTML tags content) + gensim preprocess_text()\n",
    "2. plaintext + gensim preprocess_text()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_content_dir = \"data/fulltexts_html\"\n",
    "pdf_content_dir = \"data/fulltexts_pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_sentences_from_text(text_lines, min_line_len=20, min_sen_len=15):\n",
    "    fulltext = \" \".join([line for line in list(text_lines) if len([l for l in line.split(\" \") \n",
    "                                                                   if len(l) > 0]) > min_line_len])\n",
    "    sens = filter(lambda sen: len(sen) >= min_sen_len, fulltext.split(\".\"))\n",
    "    return \". \".join(sens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /home/michal/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from gensim.parsing.preprocessing import (\n",
    "    preprocess_string,\n",
    "    remove_stopwords,\n",
    "    strip_multiple_whitespaces,\n",
    "    strip_numeric,\n",
    "    strip_punctuation,\n",
    "    strip_short,\n",
    "    strip_tags,\n",
    ")    \n",
    "import nltk\n",
    "\n",
    "\n",
    "custom_filters = [\n",
    "    lambda x: x.lower(),\n",
    "    strip_tags,\n",
    "    strip_punctuation,\n",
    "    strip_multiple_whitespaces,\n",
    "    strip_numeric,\n",
    "    remove_stopwords,\n",
    "    strip_short,\n",
    "]\n",
    "\n",
    "nltk.download('words')\n",
    "english_words = set(word.lower() for word in nltk.corpus.words.words())\n",
    "\n",
    "def get_texts_from_dir(texts_dir, filter_sen=False):\n",
    "    txt_files = os.listdir(texts_dir)\n",
    "    txt_files = [os.path.join(texts_dir, txt) for txt in txt_files]\n",
    "    texts = dict()\n",
    "    for txt_f in list(filter(lambda path: path.endswith(\".txt\"), txt_files)):\n",
    "        try:\n",
    "            if filter_sen:\n",
    "                # custom filtering based on sentences length:\n",
    "                text = filter_sentences_from_text(open(txt_f, \"r\").readlines())\n",
    "            else:\n",
    "                # no filtering:\n",
    "                text = open(txt_f, \"r\").read()\n",
    "            text = open(txt_f, \"r\").read()\n",
    "            texts[os.path.basename(txt_f)] = [\n",
    "                word for word in preprocess_string(text, custom_filters)\n",
    "                if word in english_words\n",
    "            ]\n",
    "        except UnicodeDecodeError:\n",
    "            print(\"Utf-8 decode error on %s\" % txt_f)\n",
    "            continue\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_texts_no_preproc(texts_dir):\n",
    "    txt_files = os.listdir(texts_dir)\n",
    "    txt_files = [os.path.join(texts_dir, txt) for txt in txt_files]\n",
    "    for txt_f in list(filter(lambda path: path.endswith(\".txt\"), txt_files)):\n",
    "        yield open(txt_f, \"r\").readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: using filter for short sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\ufeffToggle navigation\\xa0\\xa0\\xa0IDEALS \\n',\n",
       " '    • Login \\n',\n",
       " '    • \\n',\n",
       " '    • Search IDEALSThis Collection \\n',\n",
       " '    • query \\n',\n",
       " '      Advanced Search \\n',\n",
       " '    • \\n',\n",
       " ' \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " ' \\n',\n",
       " 'Entity-relation search: context pattern driven relation ranking\\n',\n",
       " 'Welcome to the IDEALS Repository\\n',\n",
       " '\\n',\n",
       " 'JavaScript is disabled for your browser. Some features of this site may not work without it.\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Browse\\n',\n",
       " 'IDEALS\\n']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#: comparison of a text before and after filtering:\n",
    "texts = read_texts_no_preproc(html_content_dir)\n",
    "text = list(texts)[42]\n",
    "text[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A traditional page link-based search system is not adequate for users intending to query data efficiently.  For instance, emergent phenomena reveal that some entity-based search engines, such as EntityRank, directly return answers (target entities) to users instead of web pages.  Most of the time, however, compared to searching for interested entities, users more often focus on relationships among entities.  To our knowledge, there is only one web search system that automatically extracts relations from massive unstructured corpora.  This system is referred to as OpenIE, which indeed brings us one step closer to an entity relation-based system.  Nevertheless, its system extracts only direct relations between a pair of entities and ranks simply by occurrence frequency.  The monotone pattern extraction, adopted in their relation phrase extraction model, provides high quality entity relations but also fail to return many potential true relations in the corpus, which has been explained in Section 4. 2 and affects recall significantly shown in 5.  In addition, it is difficult for users to retrieve their interested and true relations from massive relation candidate set without a qualified ranking model.  Consequently, there still exists a gap between the system and users for retrieving entity relations efficiently by a simple query.  To assist users to find their interested relations efficiently, this thesis specifically focuses on the core challenges of the ranking model.  Naturally, the quality of each relation candidate is largely relevant to its context.  Thus, to evaluate various conditions, a novel idea of context patterns driven ranking has been introduced.  After evaluating our online prototype on millions of PubMed medical abstracts, we show that our system performs better than the OpenIE system on both precision and recall.  Note that this rich and novel system is the product of a collaborative team effort comprised of the following members: Zequn Zhang, Jiarui Xu, and Varun Berry, and supervised by Professor Kevin Chang']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# same one using filtering:\n",
    "text_f = filter_sentences_from_text(text)\n",
    "text_f.split(\"\\n\")[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['S187704281631655X.txt',\n",
       " '3800a765-abs.txt',\n",
       " '978-3-319-93034-3_10.txt',\n",
       " '5679915.txt',\n",
       " 'hal-01480773.txt',\n",
       " 'ALFNLP.txt',\n",
       " '1711.txt',\n",
       " '1803.txt',\n",
       " '978-3-319-22183-0_27.txt',\n",
       " '1801.txt']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "htmls_preproc = get_texts_from_dir(html_content_dir, filter_sen=True)\n",
    "list(htmls_preproc.keys())[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: without text filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Zhang et al. - 2017 - Targeted Advertising Based on Browsing History.txt',\n",
       " 'Xu et al. - 2017 - AnswerBot automated generation of answer summary .txt',\n",
       " 'Ribón - A Framework for Semantic Similarity Measures to en.txt',\n",
       " 'Graus et al. - 2013 - yourHistory–Semantic linking for a personalized ti.txt',\n",
       " 'Ali and LaPaugh - 2013 - Enabling Author-Centric Ranking of Web Content..txt',\n",
       " 'Gordeev - 2016 - Automatic detection of verbal aggression for Russi.txt',\n",
       " 'Ver Steeg and Galstyan - 2013 - Information-theoretic measures of influence based .txt',\n",
       " 'Epasto et al. - 2017 - Bicriteria distributed submodular maximization in .txt',\n",
       " 'Cao et al. - 2018 - Searching for Truth in a Database of Statistics.txt',\n",
       " 'Catizone et al. - 2012 - LIE Leadership, Influence and Expertise..txt']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdfs_preproc = get_texts_from_dir(pdf_content_dir, filter_sen=False)\n",
    "list(pdfs_preproc.keys())[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge with htmls\n",
    "merged_texts_preproc = {**pdfs_preproc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "754"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [t for t in merged_texts_preproc.values() if len(t) > 0]\n",
    "texts_links = merged_texts_preproc.keys()\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf-idf integration\n",
    "\n",
    "Two approaches:\n",
    "\n",
    "1. use idf weights instead of BoW frequencies\n",
    "2. filter out given percentile of least important words from Docs' representation\n",
    "3. combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "tfidf = TfidfModel(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. use idf weights for top-given percentile of terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def terms_for_doc(doc_id):\n",
    "    return tfidf[corpus[doc_id]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_terms_idf_for_doc(doc_id, percentile):\n",
    "    doc_terms_ordered = sorted(terms_for_doc(doc_id), key=lambda term: term[1],  reverse=True)\n",
    "    return [term[0] for term in doc_terms_ordered[:int(len(doc_terms_ordered)*percentile)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_terms_in_tuples(doc_corpus):\n",
    "    return [(dictionary.get(tup[0]), tup[1]) for tup in doc_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_terms_idf_for_doc(doc_id, percentile):\n",
    "    doc_terms_ordered = sorted(terms_for_doc(doc_id), key=lambda term: term[1], reverse=True)\n",
    "    return [(doc_terms_ordered[i][0], doc_terms_ordered[i][1]) \n",
    "            for i in range(int(len(doc_terms_ordered)*percentile))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_tfidf_for_doc(doc_idx):\n",
    "    return [(tfidf_term_tuple[0], tfidf_term_tuple[1]) \n",
    "            for tfidf_term_tuple in tfidf_corpus[doc_idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_corpus1 = [top_terms_idf_for_doc(doc_i, 0.18) for doc_i in range(len(texts))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(395, 0.8100748966009056),\n",
       " (167, 0.3988720504597434),\n",
       " (514, 0.2500924209574005),\n",
       " (134, 0.13777225950655736),\n",
       " (529, 0.10336837201530495),\n",
       " (124, 0.09178279631047316),\n",
       " (604, 0.07934668648674739),\n",
       " (125, 0.07249263373208426),\n",
       " (49, 0.06575902668688782),\n",
       " (538, 0.06336493100830388)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_corpus1[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.interfaces.TransformedCorpus at 0x7f5892983780>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary = Dictionary(texts)\n",
    "tfidf_model = TfidfModel(dictionary=dictionary)\n",
    "tf_corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "tfidf_corpus = tfidf_model[corpus]\n",
    "tfidf_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1),\n",
       " (1, 1),\n",
       " (2, 4),\n",
       " (3, 1),\n",
       " (4, 1),\n",
       " (5, 1),\n",
       " (6, 1),\n",
       " (7, 1),\n",
       " (8, 1),\n",
       " (9, 1),\n",
       " (10, 1),\n",
       " (11, 1),\n",
       " (12, 1),\n",
       " (13, 8),\n",
       " (14, 4),\n",
       " (15, 1),\n",
       " (16, 1),\n",
       " (17, 1),\n",
       " (18, 1),\n",
       " (19, 2),\n",
       " (20, 1),\n",
       " (21, 1),\n",
       " (22, 1),\n",
       " (23, 2),\n",
       " (24, 1),\n",
       " (25, 1),\n",
       " (26, 1),\n",
       " (27, 1),\n",
       " (28, 6),\n",
       " (29, 1),\n",
       " (30, 1),\n",
       " (31, 3),\n",
       " (32, 1),\n",
       " (33, 3),\n",
       " (34, 1),\n",
       " (35, 1),\n",
       " (36, 2),\n",
       " (37, 1),\n",
       " (38, 3),\n",
       " (39, 1),\n",
       " (40, 4),\n",
       " (41, 4),\n",
       " (42, 1),\n",
       " (43, 3),\n",
       " (44, 1),\n",
       " (45, 2),\n",
       " (46, 3),\n",
       " (47, 1),\n",
       " (48, 1),\n",
       " (49, 12),\n",
       " (50, 1),\n",
       " (51, 3),\n",
       " (52, 18),\n",
       " (53, 4),\n",
       " (54, 6),\n",
       " (55, 1),\n",
       " (56, 1),\n",
       " (57, 1),\n",
       " (58, 1),\n",
       " (59, 1),\n",
       " (60, 1),\n",
       " (61, 1),\n",
       " (62, 1),\n",
       " (63, 1),\n",
       " (64, 3),\n",
       " (65, 2),\n",
       " (66, 1),\n",
       " (67, 1),\n",
       " (68, 2),\n",
       " (69, 1),\n",
       " (70, 2),\n",
       " (71, 1),\n",
       " (72, 1),\n",
       " (73, 1),\n",
       " (74, 4),\n",
       " (75, 1),\n",
       " (76, 1),\n",
       " (77, 1),\n",
       " (78, 2),\n",
       " (79, 1),\n",
       " (80, 2),\n",
       " (81, 2),\n",
       " (82, 1),\n",
       " (83, 2),\n",
       " (84, 1),\n",
       " (85, 1),\n",
       " (86, 1),\n",
       " (87, 1),\n",
       " (88, 1),\n",
       " (89, 2),\n",
       " (90, 1),\n",
       " (91, 1),\n",
       " (92, 3),\n",
       " (93, 1),\n",
       " (94, 3),\n",
       " (95, 1),\n",
       " (96, 1),\n",
       " (97, 1),\n",
       " (98, 4),\n",
       " (99, 1),\n",
       " (100, 4),\n",
       " (101, 1),\n",
       " (102, 1),\n",
       " (103, 1),\n",
       " (104, 2),\n",
       " (105, 1),\n",
       " (106, 1),\n",
       " (107, 4),\n",
       " (108, 1),\n",
       " (109, 1),\n",
       " (110, 2),\n",
       " (111, 8),\n",
       " (112, 4),\n",
       " (113, 2),\n",
       " (114, 1),\n",
       " (115, 1),\n",
       " (116, 1),\n",
       " (117, 1),\n",
       " (118, 4),\n",
       " (119, 22),\n",
       " (120, 1),\n",
       " (121, 1),\n",
       " (122, 1),\n",
       " (123, 1),\n",
       " (124, 26),\n",
       " (125, 19),\n",
       " (126, 2),\n",
       " (127, 1),\n",
       " (128, 1),\n",
       " (129, 1),\n",
       " (130, 7),\n",
       " (131, 1),\n",
       " (132, 3),\n",
       " (133, 1),\n",
       " (134, 48),\n",
       " (135, 47),\n",
       " (136, 2),\n",
       " (137, 3),\n",
       " (138, 1),\n",
       " (139, 1),\n",
       " (140, 1),\n",
       " (141, 2),\n",
       " (142, 1),\n",
       " (143, 2),\n",
       " (144, 2),\n",
       " (145, 4),\n",
       " (146, 1),\n",
       " (147, 1),\n",
       " (148, 1),\n",
       " (149, 1),\n",
       " (150, 1),\n",
       " (151, 3),\n",
       " (152, 3),\n",
       " (153, 4),\n",
       " (154, 1),\n",
       " (155, 4),\n",
       " (156, 2),\n",
       " (157, 1),\n",
       " (158, 1),\n",
       " (159, 1),\n",
       " (160, 1),\n",
       " (161, 1),\n",
       " (162, 11),\n",
       " (163, 2),\n",
       " (164, 1),\n",
       " (165, 1),\n",
       " (166, 2),\n",
       " (167, 114),\n",
       " (168, 1),\n",
       " (169, 1),\n",
       " (170, 3),\n",
       " (171, 1),\n",
       " (172, 1),\n",
       " (173, 1),\n",
       " (174, 3),\n",
       " (175, 1),\n",
       " (176, 10),\n",
       " (177, 1),\n",
       " (178, 4),\n",
       " (179, 1),\n",
       " (180, 1),\n",
       " (181, 1),\n",
       " (182, 2),\n",
       " (183, 3),\n",
       " (184, 1),\n",
       " (185, 4),\n",
       " (186, 1),\n",
       " (187, 2),\n",
       " (188, 1),\n",
       " (189, 2),\n",
       " (190, 3),\n",
       " (191, 1),\n",
       " (192, 1),\n",
       " (193, 1),\n",
       " (194, 1),\n",
       " (195, 2),\n",
       " (196, 3),\n",
       " (197, 3),\n",
       " (198, 1),\n",
       " (199, 2),\n",
       " (200, 6),\n",
       " (201, 2),\n",
       " (202, 1),\n",
       " (203, 4),\n",
       " (204, 1),\n",
       " (205, 9),\n",
       " (206, 4),\n",
       " (207, 2),\n",
       " (208, 1),\n",
       " (209, 6),\n",
       " (210, 2),\n",
       " (211, 1),\n",
       " (212, 1),\n",
       " (213, 1),\n",
       " (214, 1),\n",
       " (215, 2),\n",
       " (216, 2),\n",
       " (217, 1),\n",
       " (218, 1),\n",
       " (219, 1),\n",
       " (220, 1),\n",
       " (221, 4),\n",
       " (222, 1),\n",
       " (223, 2),\n",
       " (224, 1),\n",
       " (225, 11),\n",
       " (226, 1),\n",
       " (227, 1),\n",
       " (228, 3),\n",
       " (229, 1),\n",
       " (230, 2),\n",
       " (231, 1),\n",
       " (232, 1),\n",
       " (233, 2),\n",
       " (234, 2),\n",
       " (235, 1),\n",
       " (236, 1),\n",
       " (237, 1),\n",
       " (238, 1),\n",
       " (239, 1),\n",
       " (240, 9),\n",
       " (241, 2),\n",
       " (242, 2),\n",
       " (243, 1),\n",
       " (244, 1),\n",
       " (245, 7),\n",
       " (246, 1),\n",
       " (247, 2),\n",
       " (248, 2),\n",
       " (249, 1),\n",
       " (250, 2),\n",
       " (251, 1),\n",
       " (252, 6),\n",
       " (253, 3),\n",
       " (254, 1),\n",
       " (255, 1),\n",
       " (256, 2),\n",
       " (257, 19),\n",
       " (258, 1),\n",
       " (259, 6),\n",
       " (260, 1),\n",
       " (261, 1),\n",
       " (262, 1),\n",
       " (263, 1),\n",
       " (264, 1),\n",
       " (265, 1),\n",
       " (266, 1),\n",
       " (267, 1),\n",
       " (268, 1),\n",
       " (269, 1),\n",
       " (270, 1),\n",
       " (271, 1),\n",
       " (272, 1),\n",
       " (273, 1),\n",
       " (274, 1),\n",
       " (275, 1),\n",
       " (276, 2),\n",
       " (277, 1),\n",
       " (278, 1),\n",
       " (279, 1),\n",
       " (280, 3),\n",
       " (281, 6),\n",
       " (282, 1),\n",
       " (283, 7),\n",
       " (284, 4),\n",
       " (285, 6),\n",
       " (286, 2),\n",
       " (287, 2),\n",
       " (288, 2),\n",
       " (289, 4),\n",
       " (290, 1),\n",
       " (291, 1),\n",
       " (292, 2),\n",
       " (293, 1),\n",
       " (294, 2),\n",
       " (295, 8),\n",
       " (296, 1),\n",
       " (297, 2),\n",
       " (298, 1),\n",
       " (299, 1),\n",
       " (300, 4),\n",
       " (301, 1),\n",
       " (302, 1),\n",
       " (303, 2),\n",
       " (304, 2),\n",
       " (305, 2),\n",
       " (306, 2),\n",
       " (307, 38),\n",
       " (308, 1),\n",
       " (309, 7),\n",
       " (310, 1),\n",
       " (311, 1),\n",
       " (312, 3),\n",
       " (313, 10),\n",
       " (314, 2),\n",
       " (315, 6),\n",
       " (316, 1),\n",
       " (317, 1),\n",
       " (318, 1),\n",
       " (319, 1),\n",
       " (320, 1),\n",
       " (321, 2),\n",
       " (322, 33),\n",
       " (323, 1),\n",
       " (324, 1),\n",
       " (325, 2),\n",
       " (326, 2),\n",
       " (327, 2),\n",
       " (328, 1),\n",
       " (329, 2),\n",
       " (330, 1),\n",
       " (331, 1),\n",
       " (332, 2),\n",
       " (333, 1),\n",
       " (334, 1),\n",
       " (335, 13),\n",
       " (336, 1),\n",
       " (337, 1),\n",
       " (338, 1),\n",
       " (339, 4),\n",
       " (340, 1),\n",
       " (341, 1),\n",
       " (342, 3),\n",
       " (343, 2),\n",
       " (344, 1),\n",
       " (345, 1),\n",
       " (346, 3),\n",
       " (347, 2),\n",
       " (348, 2),\n",
       " (349, 1),\n",
       " (350, 1),\n",
       " (351, 3),\n",
       " (352, 3),\n",
       " (353, 1),\n",
       " (354, 1),\n",
       " (355, 1),\n",
       " (356, 3),\n",
       " (357, 1),\n",
       " (358, 1),\n",
       " (359, 3),\n",
       " (360, 18),\n",
       " (361, 1),\n",
       " (362, 1),\n",
       " (363, 2),\n",
       " (364, 1),\n",
       " (365, 2),\n",
       " (366, 5),\n",
       " (367, 1),\n",
       " (368, 8),\n",
       " (369, 1),\n",
       " (370, 1),\n",
       " (371, 5),\n",
       " (372, 9),\n",
       " (373, 1),\n",
       " (374, 4),\n",
       " (375, 2),\n",
       " (376, 1),\n",
       " (377, 1),\n",
       " (378, 1),\n",
       " (379, 1),\n",
       " (380, 4),\n",
       " (381, 23),\n",
       " (382, 2),\n",
       " (383, 1),\n",
       " (384, 1),\n",
       " (385, 1),\n",
       " (386, 2),\n",
       " (387, 1),\n",
       " (388, 2),\n",
       " (389, 9),\n",
       " (390, 2),\n",
       " (391, 2),\n",
       " (392, 1),\n",
       " (393, 1),\n",
       " (394, 1),\n",
       " (395, 284),\n",
       " (396, 2),\n",
       " (397, 7),\n",
       " (398, 1),\n",
       " (399, 1),\n",
       " (400, 2),\n",
       " (401, 1),\n",
       " (402, 2),\n",
       " (403, 3),\n",
       " (404, 3),\n",
       " (405, 1),\n",
       " (406, 1),\n",
       " (407, 3),\n",
       " (408, 1),\n",
       " (409, 2),\n",
       " (410, 4),\n",
       " (411, 2),\n",
       " (412, 1),\n",
       " (413, 7),\n",
       " (414, 3),\n",
       " (415, 1),\n",
       " (416, 1),\n",
       " (417, 1),\n",
       " (418, 2),\n",
       " (419, 3),\n",
       " (420, 1),\n",
       " (421, 1),\n",
       " (422, 1),\n",
       " (423, 10),\n",
       " (424, 1),\n",
       " (425, 1),\n",
       " (426, 1),\n",
       " (427, 1),\n",
       " (428, 1),\n",
       " (429, 1),\n",
       " (430, 4),\n",
       " (431, 1),\n",
       " (432, 2),\n",
       " (433, 3),\n",
       " (434, 1),\n",
       " (435, 1),\n",
       " (436, 4),\n",
       " (437, 2),\n",
       " (438, 1),\n",
       " (439, 3),\n",
       " (440, 1),\n",
       " (441, 1),\n",
       " (442, 4),\n",
       " (443, 1),\n",
       " (444, 1),\n",
       " (445, 1),\n",
       " (446, 9),\n",
       " (447, 1),\n",
       " (448, 1),\n",
       " (449, 1),\n",
       " (450, 1),\n",
       " (451, 1),\n",
       " (452, 1),\n",
       " (453, 1),\n",
       " (454, 1),\n",
       " (455, 3),\n",
       " (456, 7),\n",
       " (457, 2),\n",
       " (458, 1),\n",
       " (459, 7),\n",
       " (460, 1),\n",
       " (461, 3),\n",
       " (462, 3),\n",
       " (463, 1),\n",
       " (464, 2),\n",
       " (465, 2),\n",
       " (466, 1),\n",
       " (467, 4),\n",
       " (468, 1),\n",
       " (469, 5),\n",
       " (470, 4),\n",
       " (471, 2),\n",
       " (472, 1),\n",
       " (473, 4),\n",
       " (474, 7),\n",
       " (475, 6),\n",
       " (476, 1),\n",
       " (477, 2),\n",
       " (478, 1),\n",
       " (479, 1),\n",
       " (480, 1),\n",
       " (481, 3),\n",
       " (482, 2),\n",
       " (483, 1),\n",
       " (484, 1),\n",
       " (485, 3),\n",
       " (486, 1),\n",
       " (487, 6),\n",
       " (488, 1),\n",
       " (489, 2),\n",
       " (490, 3),\n",
       " (491, 1),\n",
       " (492, 1),\n",
       " (493, 1),\n",
       " (494, 1),\n",
       " (495, 4),\n",
       " (496, 1),\n",
       " (497, 1),\n",
       " (498, 1),\n",
       " (499, 3),\n",
       " (500, 1),\n",
       " (501, 1),\n",
       " (502, 3),\n",
       " (503, 6),\n",
       " (504, 1),\n",
       " (505, 9),\n",
       " (506, 1),\n",
       " (507, 24),\n",
       " (508, 1),\n",
       " (509, 1),\n",
       " (510, 1),\n",
       " (511, 2),\n",
       " (512, 1),\n",
       " (513, 1),\n",
       " (514, 92),\n",
       " (515, 3),\n",
       " (516, 1),\n",
       " (517, 1),\n",
       " (518, 1),\n",
       " (519, 2),\n",
       " (520, 1),\n",
       " (521, 1),\n",
       " (522, 1),\n",
       " (523, 2),\n",
       " (524, 1),\n",
       " (525, 1),\n",
       " (526, 1),\n",
       " (527, 1),\n",
       " (528, 1),\n",
       " (529, 136),\n",
       " (530, 2),\n",
       " (531, 1),\n",
       " (532, 1),\n",
       " (533, 1),\n",
       " (534, 1),\n",
       " (535, 1),\n",
       " (536, 1),\n",
       " (537, 2),\n",
       " (538, 32),\n",
       " (539, 1),\n",
       " (540, 2),\n",
       " (541, 1),\n",
       " (542, 1),\n",
       " (543, 1),\n",
       " (544, 1),\n",
       " (545, 1),\n",
       " (546, 1),\n",
       " (547, 2),\n",
       " (548, 1),\n",
       " (549, 1),\n",
       " (550, 1),\n",
       " (551, 1),\n",
       " (552, 1),\n",
       " (553, 1),\n",
       " (554, 9),\n",
       " (555, 1),\n",
       " (556, 4),\n",
       " (557, 1),\n",
       " (558, 1),\n",
       " (559, 1),\n",
       " (560, 2),\n",
       " (561, 1),\n",
       " (562, 1),\n",
       " (563, 9),\n",
       " (564, 1),\n",
       " (565, 4),\n",
       " (566, 1),\n",
       " (567, 1),\n",
       " (568, 1),\n",
       " (569, 1),\n",
       " (570, 4),\n",
       " (571, 1),\n",
       " (572, 11),\n",
       " (573, 10),\n",
       " (574, 2),\n",
       " (575, 3),\n",
       " (576, 2),\n",
       " (577, 2),\n",
       " (578, 18),\n",
       " (579, 6),\n",
       " (580, 2),\n",
       " (581, 1),\n",
       " (582, 1),\n",
       " (583, 7),\n",
       " (584, 6),\n",
       " (585, 1),\n",
       " (586, 3),\n",
       " (587, 4),\n",
       " (588, 2),\n",
       " (589, 1),\n",
       " (590, 4),\n",
       " (591, 1),\n",
       " (592, 1),\n",
       " (593, 1),\n",
       " (594, 18),\n",
       " (595, 1),\n",
       " (596, 1),\n",
       " (597, 1),\n",
       " (598, 1),\n",
       " (599, 4),\n",
       " (600, 1),\n",
       " (601, 2),\n",
       " (602, 11),\n",
       " (603, 1),\n",
       " (604, 54),\n",
       " (605, 2),\n",
       " (606, 1),\n",
       " (607, 1),\n",
       " (608, 2),\n",
       " (609, 3),\n",
       " (610, 1),\n",
       " (611, 3),\n",
       " (612, 1),\n",
       " (613, 7),\n",
       " (614, 1),\n",
       " (615, 4),\n",
       " (616, 11),\n",
       " (617, 6),\n",
       " (618, 3),\n",
       " (619, 1),\n",
       " (620, 1),\n",
       " (621, 3),\n",
       " (622, 1),\n",
       " (623, 2),\n",
       " (624, 2),\n",
       " (625, 1),\n",
       " (626, 3),\n",
       " (627, 2),\n",
       " (628, 3),\n",
       " (629, 1),\n",
       " (630, 1),\n",
       " (631, 1),\n",
       " (632, 3),\n",
       " (633, 4),\n",
       " (634, 1),\n",
       " (635, 1),\n",
       " (636, 92),\n",
       " (637, 1),\n",
       " (638, 1),\n",
       " (639, 10),\n",
       " (640, 6),\n",
       " (641, 1),\n",
       " (642, 2),\n",
       " (643, 10),\n",
       " (644, 1),\n",
       " (645, 1),\n",
       " (646, 1),\n",
       " (647, 1),\n",
       " (648, 1)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_corpus[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. use term frequencies for top-given percentile of terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_terms_freqs_for_doc(doc_id, percentile):\n",
    "    doc_terms_idf = dict(terms_for_doc(doc_id))\n",
    "    doc_terms_count = corpus[doc_id]\n",
    "    doc_terms_count_ordered = sorted(doc_terms_count, \n",
    "                                     key=lambda term_count: doc_terms_idf[term_count[0]], reverse=True)\n",
    "    return [(doc_terms_count_ordered[i][0], doc_terms_count_ordered[i][1])\n",
    "            for i in range(int(len(doc_terms_count_ordered)*percentile))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(395, 284),\n",
       " (167, 114),\n",
       " (514, 92),\n",
       " (134, 48),\n",
       " (529, 136),\n",
       " (124, 26),\n",
       " (604, 54),\n",
       " (125, 19),\n",
       " (49, 12),\n",
       " (538, 32)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_corpus2 = [top_terms_freqs_for_doc(doc_i, 0.1) for doc_i in range(len(texts))]\n",
    "tfidf_corpus2[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA computation\n",
    "\n",
    "Train LDA on a given type of corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_from_texts(given_corpus, num_topics, passes):\n",
    "    lda = LdaModel(given_corpus, num_topics=num_topics, alpha='auto', eval_every=5, passes=passes)\n",
    "    return lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_distro_for_text(text):\n",
    "    return lda.get_document_topics(dictionary.doc2bow(text), minimum_probability=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def terms_for_topic(topic_id, top_terms=10):\n",
    "    topic_top_terms = lda.get_topic_terms(topic_id, topn=top_terms)\n",
    "    return [dictionary.get(term[0]) for term in topic_top_terms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lda_num_topics = 11\n",
    "num_passes = 10\n",
    "\n",
    "# TODO: choose between corpus, tfidf_corpus1, tfidf_corpus2:\n",
    "lda = lda_from_texts(tfidf_corpus2, num_topics=lda_num_topics, passes=num_passes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Documents representation\n",
    "\n",
    "Each doc is represented as a probability distribution towards LDA topics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1257"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add HTML texts to a set of all texts\n",
    "merged_texts_preproc = {**pdfs_preproc, **htmls_preproc}\n",
    "texts = [t for t in merged_texts_preproc.values() if len(t) > 0]\n",
    "texts_links = merged_texts_preproc.keys()\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_distros = np.array([topic_distro_for_text(text) for text in texts[:-1]])\n",
    "topic_distros = topic_distros[:, :, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1256, 11)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_distros.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization projections: approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get a projection of docs so that the docs are close to their major topic\n",
    "\n",
    "Doc distance to a topic must be proportional to it's probability of belonging to it\n",
    "\n",
    "Does not consider relative distance of the documents, which seems to me as having no interpretation value - if it does, it is again equal to the second approach\n",
    "\n",
    "... thus is not implemented\n",
    "\n",
    "Yet, other projection methods surely deserve a consideration\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get a projection of topic according to their relative similarity\n",
    "\n",
    "Relative similarity is a correlation of documents' belonging to it\n",
    "\n",
    "Topics centers are documents with one-hot distribution of probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_topic_docs_distros = np.identity(lda_num_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_distros = np.append(topic_distros, base_topic_docs_distros) \\\n",
    "                  .reshape((len(topic_distros)+lda_num_topics, lda_num_topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1267, 11)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_distros.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 1.18377963, 1.08430097, ..., 0.65466782, 1.12858401,\n",
       "        1.16681353],\n",
       "       [1.18377963, 0.        , 0.276056  , ..., 1.00711261, 0.03171644,\n",
       "        1.04157268],\n",
       "       [1.08430097, 0.276056  , 0.        , ..., 1.17678818, 0.26014944,\n",
       "        0.59088021],\n",
       "       ...,\n",
       "       [0.65466782, 1.00711261, 1.17678818, ..., 0.        , 1.1       ,\n",
       "        1.1       ],\n",
       "       [1.12858401, 0.03171644, 0.26014944, ..., 1.1       , 0.        ,\n",
       "        1.1       ],\n",
       "       [1.16681353, 1.04157268, 0.59088021, ..., 1.1       , 1.1       ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# distance matrix by selected metric\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "dists = squareform(pdist(topic_distros, metric=\"correlation\"))\n",
    "dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1267, 1267)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dists.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MDS(dissimilarity='precomputed', eps=0.001, max_iter=300, metric=True,\n",
       "  n_components=2, n_init=4, n_jobs=None, random_state=None, verbose=0)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# projection to 2D using MDS\n",
    "\n",
    "from sklearn import manifold\n",
    "\n",
    "adist = dists\n",
    "\n",
    "amax = np.amax(adist)\n",
    "adist /= amax\n",
    "\n",
    "mds = manifold.MDS(n_components=2, dissimilarity=\"precomputed\")\n",
    "results = mds.fit(adist)\n",
    "\n",
    "coords = results.embedding_\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matplotlib visualization:\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# from matplotlib.pyplot import figure\n",
    "\n",
    "\n",
    "# plt.subplots_adjust(bottom = 0.1)\n",
    "# plt.scatter(\n",
    "#     coords[:, 0], coords[:, 1], marker = 'o'\n",
    "#     )\n",
    "# for label, x, y in zip([\"\"]*len(coords), coords[:, 0], coords[:, 1]):\n",
    "#     plt.annotate(\n",
    "#         label,\n",
    "#         xy = (x, y), xytext = (-20, 20),\n",
    "#         textcoords = 'offset points', ha = 'right', va = 'bottom',\n",
    "#         bbox = dict(boxstyle = 'round,pad=0.5', fc = 'yellow', alpha = 0.5),\n",
    "#         arrowprops = dict(arrowstyle = '->', connectionstyle = 'arc3,rad=0'))\n",
    "    \n",
    "# # figure(num=None, figsize=(10, 10), dpi=80, facecolor='w', edgecolor='k')\n",
    "# plt.figsize = (100, 100)\n",
    "# plt.grid()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selective labeling of topics by only the unique words among topics\n",
    "\n",
    "word_occurrence_bound = int(lda.num_topics / 2)\n",
    "top_terms_per_topic = 30\n",
    "max_output_words = 10\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "all_topics_w = [terms_for_topic(i, top_terms=top_terms_per_topic) for i in range(lda_num_topics)]\n",
    "all_words = reduce(lambda x, y: set(x) | set(y), all_topics_w)\n",
    "intersect_words = list(filter(lambda w: sum([w in t_words for t_words in all_topics_w]) > word_occurrence_bound, \n",
    "                              all_words))\n",
    "unique_topics_w = [[w for w in t_words if w not in intersect_words] for t_words in all_topics_w]\n",
    "unique_topics_w = [twords[:max_output_words] for twords in all_topics_w if len(twords) > max_output_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly\n",
    "plotly.tools.set_credentials_file(username='stefanik.m', api_key='ChJP5J2dPZgTtv3p6DgH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High five! You successfully sent some data to your account on plotly. View your plot in your browser at https://plot.ly/~stefanik.m/0 or inside your plot.ly account where it is named 'TODO: fill appropriately: MDS over LDA X topics. tfidf:counts for top X terms as frequencies.'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~stefanik.m/0.embed\" height=\"1000px\" width=\"1000px\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "docs_len = len(texts)\n",
    "\n",
    "# Documents trace\n",
    "trace_docs = go.Scatter(\n",
    "    x = coords[:docs_len, 0],\n",
    "    y = coords[:docs_len, 1],\n",
    "    mode = 'markers',\n",
    "    marker = dict(color = 'rgba(0, 0, 255, .5)', size = 5),\n",
    "    text = list(texts_links)\n",
    ")\n",
    "# Bases (topics documents) trace\n",
    "trace_bases = go.Scatter(\n",
    "    x = coords[docs_len:, 0],\n",
    "    y = coords[docs_len:, 1],\n",
    "    mode = 'markers',\n",
    "    marker = dict(color = 'rgba(255, 0, 122, .2)', size = 60),\n",
    "    text = [\"T %s: %s\" % (i, unique_topics_w[i]) for i in range(lda_num_topics)]\n",
    ")\n",
    "\n",
    "data = [trace_docs, trace_bases]\n",
    "\n",
    "# label = 'MDS over LDA %s topics. tfidf for top 0.5 terms as frequencies - TODO: check' % lda.num_topics\n",
    "\n",
    "label = 'TODO: fill appropriately: MDS over LDA X topics. tfidf:counts for top X terms as frequencies.'\n",
    "\n",
    "\n",
    "# Plot and embed in ipython notebook!\n",
    "layout = dict(title=label,  \n",
    "              font=dict(size=12),\n",
    "              showlegend=True,\n",
    "              width=1000,\n",
    "              height=1000,\n",
    "              margin=dict(l=40, r=40, b=85, t=100),\n",
    "              hovermode='closest',\n",
    "              plot_bgcolor='rgb(256,256,256)'          \n",
    "              )\n",
    "py.iplot(dict(data=data, layout=layout), filename=label) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
